# Processing (Apache Spark)
https://spark-summit.org/2014/training/

## 1. Introducing Apache Spark
- Spark Summit 2014 행사 중 Databricks에서 2014년에 발표한 Apache Spark에 대한 소개자료
- Spark에 대한 기본 개념에서 Streaming, SQL, MLlib, GraphX까지 전반적인 개념파악에 도움
- 강의 slice : http://training.databricks.com/workshop/itas_workshop.pdf
- 강의 영상 : https://youtu.be/VWeWViFCzzg?list=PLTPXxbhUt-YWSgAUhrnkyphnh0oKIT8-j
- 주요 내용

```
The Introduction to Apache Spark workshop is for users to learn the core Spark APIs. This session features hands-on technical exercises to get developers up to speed in using Spark for data exploration, analysis, and building big data applications.

The integrated lecture and lab format covers the following topics:

 - Overview of Big Data and Spark
 - Installing Spark Locally
 - Using Spark’s Core APIs in Scala, Java, & Python
 - Building Spark Applications
 - Deploying on a Big Data Cluster
 - Building Applications for Multiple Platforms
```


## 2. Advanced Apache Spark
- Spark의 핵심 모듈에 대하여 좀 더 세부적으로 설명하는 자료
- 실시간 처리를 위한 streaming 부분과 머신러닝을 위한 MLlib를 참고하면 좋음.
- 실습 자료 : https://databricks-training.s3.amazonaws.com/index.html
- 주요 내용

```
The Advanced Apache Spark Workshop will cover advanced topics on architecture, tuning, and each of Spark’s high-level libraries (including the latest features). Attendees will have the opportunity after the lunch break to work through labs on each of the libraries.

Some familiarity with Spark or MapReduce is expected, as this workshop will not cover basic Spark programming.

Topics covered include:

 - Advanced Spark Internals and Tuning – Reynold Xin – SLIDES | VIDEO
 - Spark SQL – Michael Armburst – SLIDES | VIDEO
 - Spark Streaming – https://databricks-training.s3.amazonaws.com/slides/Spark%20Summit%202014%20-%20Spark%20Streaming.pdf
 - MLlib – https://databricks-training.s3.amazonaws.com/slides/Spark_Summit_MLlib_070214_v2.pdf
 - GraphX – Ankur Dave – SLIDES | VIDEO
```

## 3. 실습 프로젝트
- https://github.com/freepsw/simple-streaming
- kafka -> apache spark(streaming, mllib)-> (redis, elasticsearch) -> (kibana, nodejs)
- 위의 시나리오에 따라 동작하는 프로그램 개발 (작업 중)

### 3.1 install and run spark
#### install apache spark 2.1.1
```
> cd ~/apps
> wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz
> tar xvf spark-2.1.1-bin-hadoop2.7.tgz

# spark cluster를 구성할 worker node를 추가한다.
# 실습에서는 1대에 master와 worker를 1개씩 설치하므로, localhost만 추가한다.
> cp slaves.template slaves
> vi slaves
localhost
```

#### run spark Cluster
```
> cd ~apps/spark-2.1.1-bin-hadoop2.7
> sbin/start-all.sh
```
- http://<ip>:8080 접속


- Error 발생
 * "localhost: Permission denied (publickey,gssapi-keyex,gssapi-with-mic)."
- 원인
 * worker를 구동하기 위하여 ssh로 접속해야 하는데,
 * 현재 ssh로 접속하기 위한 ssh key가 존재하지 않음
- 해결
```
> ssh-keygen -t rsa -P ""
> cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
```

### 3.2 run spark streaming applications
#### clone git repository
- 아래 github에서 제공하는 코드를 사용하여 실습 진행
```
> sudo yum install -y git
> cd apps
> git clone https://github.com/PacktPublishing/Apache-Spark-2-for-Beginners.git
```

### 3.3 run socket event processing example
#### install sbt compiler
- http://www.scala-sbt.org/release/docs/Installing-sbt-on-Linux.html 참고
```
> curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
> sudo yum install sbt
```

#### compile scala version
```
> cd ~apps/Apache-Spark-2-for-Beginners/
> cd Code_Chapter\ 6/Code/Scala
# shell을 실행할 수 있는 권한 설정
> chmod +x *.sh

# config.sbt 수정
> vi config.sbt
val sparkVersion = "2.1.1"  --> SNAPSHOT 제거
> ./compile.sh
```

#### run spark streaming application
- 환경변수 등록 (SPARK_HOME)
```
> vi ~/.bash_profile
export SPARK_HOME=/home/freepsw.01/apps/spark-2.1.1-bin-hadoop2.7
> source ~/.bash_profile
```

- spark application 실행
```
- 0은 kafka를 사용하는지 구분하는 단어 (1인 경우 kafka 적용)
> ./submit.sh com.packtpub.sfb.StreamingApps 0
```

- netcat 실행
```
> nc -lk 9999
[Wed Oct 11 14:32:52 2000] [error] [client 127.0.0.1] client denied by server configuration: /export/home/live/ap/htdocs/test
```

- 이렇게 실행하면 spark application 로그에 해당 문자가 출력됨
